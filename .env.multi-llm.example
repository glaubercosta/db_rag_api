# Multi-LLM Configuration Example
# Copy this to .env and configure your providers

# ================================
# DATABASE CONFIGURATION
# ================================
DATABASE_URL=sqlite:///./sample_data.db
DATABASE_TYPE=sqlite

# ================================
# LLM PROVIDER PREFERENCES
# ================================
# Set your preferred providers (optional)
PREFERRED_LLM_PROVIDER=openai
PREFERRED_EMBEDDING_PROVIDER=openai

# =============================================================================
# CONFIGURAÇÃO MULTI-LLM - DATABASE RAG SYSTEM
# =============================================================================
# Copie este arquivo para .env e configure os provedores que desejar usar
# Pelo menos um provedor deve estar configurado

# =============================================================================
# CONFIGURAÇÃO GERAL DO SISTEMA
# =============================================================================

# Chave de autenticação da API (gere uma chave segura)
API_KEY=dev-multi-llm-key-12345

# Configuração do banco de dados
DATABASE_TYPE=sqlite
DATABASE_PATH=./data/example.db

# Para PostgreSQL (opcional)
# DATABASE_TYPE=postgresql
# DATABASE_URL=postgresql://user:password@localhost:5432/database

# =============================================================================
# PROVEDOR OPENAI (GPT-4, GPT-3.5, etc.)
# =============================================================================

# Chave da API OpenAI (obrigatório para usar OpenAI)
OPENAI_API_KEY=sk-sua-chave-openai-aqui

# Modelos OpenAI (opcional - valores padrão mostrados)
OPENAI_MODEL=gpt-4
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# PROVEDOR OLLAMA (MODELOS LOCAIS)
# =============================================================================

# URL base do servidor Ollama (padrão: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Modelos Ollama (baixe com: ollama pull <modelo>)
OLLAMA_MODEL=llama2
OLLAMA_EMBEDDING_MODEL=llama2

# Modelos alternativos disponíveis:
# - llama2, llama2:13b, llama2:70b
# - codellama, codellama:13b
# - mistral, mistral:7b
# - vicuna, vicuna:13b
# - orca-mini
# - neural-chat

# =============================================================================
# PROVEDOR API CUSTOMIZADA (EMPRESA/PROPRIETÁRIO)
# =============================================================================

# URL base da sua API customizada
CUSTOM_LLM_API_BASE=https://sua-empresa.com/api

# Modelo da sua API
CUSTOM_LLM_MODEL=seu-modelo-proprietario

# Chave de autenticação da sua API (se necessária)
CUSTOM_LLM_API_KEY=sua-chave-empresa-aqui

# Formato da API: "openai" (compatível OpenAI) ou "ollama" (estilo Ollama)
CUSTOM_LLM_FORMAT=openai

# Endpoints da API (opcional - valores padrão por formato)
# Para formato OpenAI:
CUSTOM_LLM_ENDPOINT=/v1/chat/completions
CUSTOM_LLM_EMBEDDING_ENDPOINT=/v1/embeddings

# Para formato Ollama:
# CUSTOM_LLM_ENDPOINT=/api/generate
# CUSTOM_LLM_EMBEDDING_ENDPOINT=/api/embeddings

# Headers customizados da API (opcional)
# Use o padrão CUSTOM_LLM_HEADER_<NOME>=<VALOR>
# Exemplo:
CUSTOM_LLM_HEADER_X_API_VERSION=2023-12-01
CUSTOM_LLM_HEADER_X_TENANT_ID=sua-empresa
CUSTOM_LLM_HEADER_X_CLIENT_VERSION=1.0.0

# =============================================================================
# PREFERÊNCIAS DOS PROVEDORES
# =============================================================================

# Provedor preferido para LLM (geralmente o mais preciso)
# Valores possíveis: openai, ollama, custom
PREFERRED_LLM_PROVIDER=openai

# Provedor preferido para embeddings (pode ser diferente do LLM)
# Valores possíveis: openai, ollama, custom
PREFERRED_EMBEDDING_PROVIDER=openai

# =============================================================================
# CONFIGURAÇÕES AVANÇADAS
# =============================================================================

# Timeout para requisições HTTP (segundos)
HTTP_TIMEOUT=30

# Tamanho máximo do context window (tokens)
MAX_CONTEXT_LENGTH=4000

# Número máximo de tentativas em caso de falha
MAX_RETRIES=3

# Tempo de cache para embeddings (segundos)
EMBEDDING_CACHE_TTL=3600

# =============================================================================
# LOGS E DEBUGGING
# =============================================================================

# Nível de log: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Arquivo de log (opcional - deixe vazio para usar apenas console)
LOG_FILE=

# Habilitar logs detalhados dos provedores
ENABLE_PROVIDER_LOGGING=false

# =============================================================================
# CONFIGURAÇÕES DE DESENVOLVIMENTO
# =============================================================================

# Porta da API (padrão: 9000 para multi-LLM, 8000 para API original)
API_PORT=9000

# Host da API
API_HOST=0.0.0.0

# Modo de desenvolvimento
DEV_MODE=true

# Recarregar automaticamente em mudanças de código
AUTO_RELOAD=true

# =============================================================================
# EXEMPLOS DE CONFIGURAÇÃO POR CENÁRIO
# =============================================================================

# CENÁRIO 1: DESENVOLVIMENTO LOCAL (apenas Ollama)
# Descomente as linhas abaixo e comente as outras configurações
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama2
# OLLAMA_EMBEDDING_MODEL=llama2
# PREFERRED_LLM_PROVIDER=ollama
# PREFERRED_EMBEDDING_PROVIDER=ollama

# CENÁRIO 2: PRODUÇÃO COM FALLBACK (OpenAI + Custom)
# OPENAI_API_KEY=sk-prod-key
# CUSTOM_LLM_API_BASE=https://empresa.com/api
# CUSTOM_LLM_API_KEY=prod-key
# PREFERRED_LLM_PROVIDER=openai
# PREFERRED_EMBEDDING_PROVIDER=custom

# CENÁRIO 3: APENAS DADOS INTERNOS (Custom API + Ollama)
# OLLAMA_BASE_URL=http://internal-ollama:11434
# CUSTOM_LLM_API_BASE=https://internal-api.com
# PREFERRED_LLM_PROVIDER=custom
# PREFERRED_EMBEDDING_PROVIDER=ollama

# CENÁRIO 4: OTIMIZAÇÃO DE CUSTOS (Ollama LLM + OpenAI Embeddings)
# OLLAMA_MODEL=llama2
# OPENAI_API_KEY=sk-key
# PREFERRED_LLM_PROVIDER=ollama
# PREFERRED_EMBEDDING_PROVIDER=openai

# ================================
# OLLAMA CONFIGURATION (LOCAL MODELS)
# ================================
# Uncomment to use Ollama for local models
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama2
# OLLAMA_EMBEDDING_MODEL=llama2
# OLLAMA_TEMPERATURE=0.0

# ================================
# CUSTOM/ENTERPRISE LLM CONFIGURATION
# ================================
# For your company's internal LLM APIs
# CUSTOM_LLM_API_BASE=https://your-company-llm-api.com
# CUSTOM_LLM_MODEL=your-enterprise-model
# CUSTOM_LLM_API_KEY=your-enterprise-api-key
# CUSTOM_LLM_FORMAT=openai  # "openai" or "ollama"
# CUSTOM_LLM_ENDPOINT=/v1/chat/completions
# CUSTOM_LLM_TEMPERATURE=0.0

# ================================
# CUSTOM/ENTERPRISE EMBEDDING CONFIGURATION
# ================================
# For your company's internal embedding APIs
# CUSTOM_EMBEDDING_API_BASE=https://your-company-embedding-api.com
# CUSTOM_EMBEDDING_MODEL=your-enterprise-embedding-model
# CUSTOM_EMBEDDING_ENDPOINT=/v1/embeddings

# ================================
# CUSTOM HEADERS FOR ENTERPRISE APIS
# ================================
# Add custom headers with CUSTOM_LLM_HEADER_ prefix
# Example: CUSTOM_LLM_HEADER_X_API_VERSION=2023-12-01
# This becomes header: X-API-Version: 2023-12-01

# ================================
# RAG SYSTEM CONFIGURATION
# ================================
VECTOR_STORE_PATH=./vector_store
SIMILARITY_SEARCH_K=5
TABLE_SAMPLE_LIMIT=1000

# Force rebuild vector store on startup
# RAG_FORCE_REBUILD=false
