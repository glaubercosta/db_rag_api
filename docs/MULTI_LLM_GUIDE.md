# ğŸ¤– Multi-LLM Database RAG System

## ğŸ¯ VisÃ£o Geral

O sistema foi expandido para suportar **mÃºltiplos provedores de LLM**, permitindo flexibilidade total na escolha de modelos de IA:

- **ğŸŒ OpenAI**: GPT-4, GPT-3.5, embeddings
- **ğŸ  Ollama**: Modelos locais (Llama, Mistral, CodeLlama, etc.)
- **ğŸ¢ APIs Customizadas**: Modelos proprietÃ¡rios da sua empresa
- **ğŸ”„ AlternÃ¢ncia DinÃ¢mica**: Mude de provedor em tempo real

## ğŸš€ InÃ­cio RÃ¡pido

### 1. ConfiguraÃ§Ã£o BÃ¡sica

```bash
# Clone e configure o projeto
git clone <repo-url>
cd db_rag_api

# Copie o arquivo de configuraÃ§Ã£o
cp .env.multi-llm.example .env

# Configure pelo menos um provedor no .env
```

### 2. OpÃ§Ãµes de Provedores

#### OpenAI (Nuvem)

```bash
export OPENAI_API_KEY="sk-sua-chave-aqui"
export OPENAI_MODEL="gpt-4"
export OPENAI_EMBEDDING_MODEL="text-embedding-ada-002"
```

#### Ollama (Local)

```bash
# Instale o Ollama: https://ollama.ai
ollama serve
ollama pull llama2

export OLLAMA_MODEL="llama2"
export OLLAMA_EMBEDDING_MODEL="llama2"
export OLLAMA_BASE_URL="http://localhost:11434"
```

#### API Customizada (Empresa)

```bash
export CUSTOM_LLM_API_BASE="https://sua-empresa.com/api"
export CUSTOM_LLM_MODEL="seu-modelo-proprietario"
export CUSTOM_LLM_API_KEY="sua-chave-empresa"
export CUSTOM_LLM_FORMAT="openai"  # ou "ollama"
```

### 3. Executar a API

```bash
# API Multi-LLM (nova)
python multi_llm_api.py

# A API estarÃ¡ disponÃ­vel em http://localhost:9000
```

## ğŸ“‹ Funcionalidades

### âœ¨ **Flexibilidade de Provedores**

- **Suporte simultÃ¢neo** a mÃºltiplos provedores
- **Fallback automÃ¡tico** se um provedor falhar
- **PreferÃªncias configurÃ¡veis** por tipo (LLM/Embeddings)

### ğŸ”„ **AlternÃ¢ncia DinÃ¢mica**

```python
# Via cÃ³digo Python
rag_system.switch_llm_provider("ollama")
rag_system.switch_embedding_provider("openai")

# Via API REST
POST /switch-provider
{
    "provider": "ollama",
    "type": "llm"
}
```

### ğŸ“Š **Monitoramento Completo**

```python
# Status do sistema
GET /status

# Lista todos os provedores
GET /providers
```

### ğŸ¯ **Consultas com Override**

```python
# Especifique o provedor por consulta
POST /query
{
    "query": "Quantos usuÃ¡rios existem?",
    "provider": "ollama"  # Opcional
}
```

## ğŸ› ï¸ Exemplos de Uso

### Python Direto

```python
from src.multi_llm_rag_system import create_multi_llm_rag_system_from_env

# Criar sistema a partir do ambiente
rag_system = create_multi_llm_rag_system_from_env()

# Inicializar
if rag_system.initialize():
    # Consultar
    result = rag_system.query("Mostre as vendas do Ãºltimo mÃªs")
    print(result["answer"])
    
    # Alternar provedor
    rag_system.switch_llm_provider("ollama")
    
    # Nova consulta com provedor diferente
    result2 = rag_system.query("Quantos produtos temos?")
    print(result2["answer"])
```

### API REST

```bash
# Consulta bÃ¡sica
curl -X POST http://localhost:9000/query \
  -H "Authorization: Bearer dev-multi-llm-key-12345" \
  -H "Content-Type: application/json" \
  -d '{"query": "Quantos clientes ativos temos?"}'

# Consulta com provedor especÃ­fico
curl -X POST http://localhost:9000/query \
  -H "Authorization: Bearer dev-multi-llm-key-12345" \
  -H "Content-Type: application/json" \
  -d '{"query": "Analyze user behavior", "provider": "openai"}'

# Verificar status
curl -H "Authorization: Bearer dev-multi-llm-key-12345" \
  http://localhost:9000/status

# Alternar provedor
curl -X POST http://localhost:9000/switch-provider \
  -H "Authorization: Bearer dev-multi-llm-key-12345" \
  -H "Content-Type: application/json" \
  -d '{"provider": "ollama", "type": "llm"}'
```

## ğŸ—ï¸ Arquitetura

### Componentes Principais

```text
Multi-LLM RAG System
â”œâ”€â”€ ğŸ”Œ Provider Manager
â”‚   â”œâ”€â”€ OpenAI Provider
â”‚   â”œâ”€â”€ Ollama Provider  
â”‚   â””â”€â”€ Custom Provider
â”œâ”€â”€ ğŸ§  LLM Interface
â”‚   â”œâ”€â”€ Text Generation
â”‚   â””â”€â”€ Chat Completion
â”œâ”€â”€ ğŸ“Š Embedding Interface
â”‚   â”œâ”€â”€ Document Embeddings
â”‚   â””â”€â”€ Query Embeddings
â”œâ”€â”€ ğŸ—ƒï¸ Vector Store Manager
â”œâ”€â”€ ğŸ” SQL Agent (Multi-LLM)
â””â”€â”€ ğŸ“‹ RAG Query Processor
```

### Fluxo de Processamento

1. **ConfiguraÃ§Ã£o**: Sistema detecta provedores disponÃ­veis
2. **SeleÃ§Ã£o**: Escolhe provedor ativo baseado em preferÃªncias
3. **Consulta**: Processa linguagem natural usando provedor ativo
4. **Embeddings**: Busca contexto relevante no vector store
5. **GeraÃ§Ã£o SQL**: Cria queries usando LLM ativo
6. **Resposta**: Retorna resultado com informaÃ§Ãµes do provedor

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### PreferÃªncias de Provedor

```bash
# Defina preferÃªncias globais
export PREFERRED_LLM_PROVIDER="openai"
export PREFERRED_EMBEDDING_PROVIDER="custom"

# Sistema tentarÃ¡ usar OpenAI para LLM e API customizada para embeddings
```

### Headers Customizados (Empresariais)

```bash
# Para APIs que requerem headers especÃ­ficos
export CUSTOM_LLM_HEADER_X_API_VERSION="2023-12-01"
export CUSTOM_LLM_HEADER_X_TENANT_ID="sua-empresa"

# Estes se tornam headers HTTP:
# X-API-Version: 2023-12-01
# X-Tenant-ID: sua-empresa
```

### Formatos de API Customizada

```bash
# Para APIs compatÃ­veis com OpenAI
export CUSTOM_LLM_FORMAT="openai"
export CUSTOM_LLM_ENDPOINT="/v1/chat/completions"

# Para APIs no estilo Ollama
export CUSTOM_LLM_FORMAT="ollama"
export CUSTOM_LLM_ENDPOINT="/api/generate"
```

## ğŸ” SeguranÃ§a

### AutenticaÃ§Ã£o da API

```bash
# Defina sua chave personalizada
export API_KEY="sua-chave-super-secreta"

# Use nos requests
curl -H "Authorization: Bearer sua-chave-super-secreta" ...
```

### SeguranÃ§a de Dados

- **Embeddings locais**: Use Ollama para manter dados internos
- **APIs privadas**: Configure endpoints internos da empresa
- **Logs auditoria**: Sistema registra todas as consultas e trocas de provedor

## ğŸ¯ Casos de Uso

### 1. **Desenvolvimento e ProduÃ§Ã£o**
```bash
# Desenvolvimento: Ollama (grÃ¡tis, local)
export OLLAMA_MODEL="llama2"

# ProduÃ§Ã£o: OpenAI (mais preciso)
export OPENAI_API_KEY="sk-prod-key"
export PREFERRED_LLM_PROVIDER="openai"
```

### 2. **Conformidade de Dados**
```bash
# Dados sensÃ­veis: apenas modelos locais
export OLLAMA_MODEL="llama2"  
export OLLAMA_EMBEDDING_MODEL="llama2"

# Dados pÃºblicos: pode usar OpenAI
export PREFERRED_LLM_PROVIDER="ollama"
```

### 3. **Fallback e RedundÃ¢ncia**
```bash
# Configure mÃºltiplos provedores
export OPENAI_API_KEY="sk-key"
export OLLAMA_MODEL="llama2"
export CUSTOM_LLM_API_BASE="https://backup-api.com"

# Sistema alternarÃ¡ automaticamente se um falhar
```

### 4. **OtimizaÃ§Ã£o de Custos**
```bash
# Embeddings (baratos): OpenAI
export OPENAI_API_KEY="sk-key"
export PREFERRED_EMBEDDING_PROVIDER="openai"

# LLM (caros): Ollama local
export OLLAMA_MODEL="llama2"  
export PREFERRED_LLM_PROVIDER="ollama"
```

## ğŸ“Š Monitoramento

### Status do Sistema
```python
# Ver todos os provedores e seu status
status = rag_system.get_system_info()
print(status["providers"])

# SaÃ­da exemplo:
{
    "llm_providers": {
        "openai": {"available": True, "active": True, "model": "gpt-4"},
        "ollama": {"available": False, "active": False},
        "custom": {"available": True, "active": False}
    },
    "embedding_providers": {
        "openai": {"available": True, "active": True, "dimension": 1536}
    }
}
```

### Logs e Auditoria
```python
# Sistema registra automaticamente:
# - Provedores inicializados
# - Trocas de provedor
# - Falhas de provedor
# - Queries processadas
```

## ğŸš¦ SoluÃ§Ã£o de Problemas

### âŒ Nenhum provedor disponÃ­vel
```bash
# Verifique configuraÃ§Ãµes
python examples/multi_llm_usage.py

# Verifique providers especÃ­ficos:
# OpenAI: chave vÃ¡lida?
# Ollama: serviÃ§o rodando? Modelo baixado?
# Custom: API acessÃ­vel? Headers corretos?
```

### âš ï¸ Provedor especÃ­fico falha
```bash
# Sistema alternarÃ¡ automaticamente para fallback
# Logs mostrarÃ£o o motivo da falha
```

### ğŸ”„ Troca de provedor nÃ£o funciona
```bash
# Verifique se provedor estÃ¡ disponÃ­vel
curl -H "Authorization: Bearer key" http://localhost:9000/providers

# Verifique se o nome estÃ¡ correto: "openai", "ollama", "custom"
```

## ğŸ‰ PrÃ³ximos Passos

1. **Configure seu primeiro provedor** (OpenAI ou Ollama)
2. **Execute** `python multi_llm_api.py`
3. **Teste** com `examples/multi_llm_usage.py`
4. **Configure provedores adicionais** para redundÃ¢ncia
5. **Implemente** em produÃ§Ã£o com suas APIs customizadas

---

**ğŸ¯ Resultado**: Sistema flexÃ­vel que se adapta Ã s suas necessidades, desde desenvolvimento local atÃ© produÃ§Ã£o empresarial com APIs proprietÃ¡rias!
