# ğŸ¤– Multi-LLM Database RAG System

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green)](https://fastapi.tiangolo.com)
[![Multi-LLM](https://img.shields.io/badge/Multi--LLM-OpenAI%20%7C%20Ollama%20%7C%20Custom-purple)](https://github.com)

Um sistema RAG (Retrieval-Augmented Generation) de Ãºltima geraÃ§Ã£o que suporta **mÃºltiplos provedores de LLM** para consultas inteligentes de banco de dados.

## ğŸ¯ VisÃ£o Geral

O **Multi-LLM Database RAG System** revoluciona a forma como vocÃª interage com seus dados, oferecendo:

### ğŸ§  **Flexibilidade de IA**
- **OpenAI**: GPT-4, GPT-3.5 (precisÃ£o e qualidade)
- **Ollama**: Modelos locais (privacidade e controle)
- **APIs Customizadas**: Modelos proprietÃ¡rios da empresa

### ğŸ”„ **Adaptabilidade Total**
- AlternÃ¢ncia dinÃ¢mica entre provedores
- Fallback automÃ¡tico em caso de falha
- ConfiguraÃ§Ã£o por ambiente (dev/prod)
- PreferÃªncias por tipo (LLM/Embeddings)

### ğŸ¢ **Pronto para Empresa**
- Suporte a APIs internas
- Headers customizados
- AutenticaÃ§Ã£o flexÃ­vel
- Logs de auditoria

## ğŸš€ InÃ­cio em 3 Minutos

### 1ï¸âƒ£ **InstalaÃ§Ã£o**

```bash
git clone <seu-repo> && cd db_rag_api
python -m venv venv && venv\\Scripts\\activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ **ConfiguraÃ§Ã£o RÃ¡pida**

```bash
# Copie o template de configuraÃ§Ã£o
cp .env.multi-llm.example .env

# Configure pelo menos UM provedor:
```

**OpÃ§Ã£o A - OpenAI (Mais FÃ¡cil):**
```bash
echo "OPENAI_API_KEY=sk-sua-chave-aqui" >> .env
```

**OpÃ§Ã£o B - Ollama (Gratuito, Local):**
```bash
# Instale Ollama: https://ollama.ai
ollama serve & ollama pull llama2
echo "OLLAMA_MODEL=llama2" >> .env
echo "PREFERRED_LLM_PROVIDER=ollama" >> .env
```

**OpÃ§Ã£o C - API da Empresa:**
```bash
echo "CUSTOM_LLM_API_BASE=https://sua-empresa.com/api" >> .env
echo "CUSTOM_LLM_MODEL=seu-modelo" >> .env
echo "PREFERRED_LLM_PROVIDER=custom" >> .env
```

### 3ï¸âƒ£ **Executar e Testar**

```bash
# Teste rÃ¡pido do sistema
python quick_test.py

# Executar API Multi-LLM
python multi_llm_api.py

# âœ… API disponÃ­vel em: http://localhost:9000
```

## ğŸ’¡ Exemplos PrÃ¡ticos

### ğŸ” **Consulta Via API**

```bash
curl -X POST http://localhost:9000/query \\
  -H "Authorization: Bearer dev-multi-llm-key-12345" \\
  -H "Content-Type: application/json" \\
  -d '{"query": "Quantos usuÃ¡rios ativos temos este mÃªs?"}'
```

### ğŸ **Uso Direto em Python**

```python
from src.multi_llm_rag_system import create_multi_llm_rag_system_from_env

# Criar sistema a partir das variÃ¡veis de ambiente
rag = create_multi_llm_rag_system_from_env()

# Inicializar
if rag.initialize():
    # Consultar
    result = rag.query("Mostre as vendas por regiÃ£o")
    print(result["answer"])
    
    # Trocar provedor dinamicamente
    rag.switch_llm_provider("ollama")
    
    # Nova consulta com provedor diferente
    result2 = rag.query("AnÃ¡lise de performance")
    print(f"Resposta via {result2['provider_used']}: {result2['answer']}")
```

### âš¡ **AlternÃ¢ncia de Provedor**

```bash
# Mudar para Ollama (local)
curl -X POST http://localhost:9000/switch-provider \\
  -H "Authorization: Bearer dev-multi-llm-key-12345" \\
  -H "Content-Type: application/json" \\
  -d '{"provider": "ollama", "type": "llm"}'

# Consulta especÃ­fica com provedor
curl -X POST http://localhost:9000/query \\
  -H "Authorization: Bearer dev-multi-llm-key-12345" \\
  -H "Content-Type: application/json" \\
  -d '{"query": "AnÃ¡lise de dados", "provider": "openai"}'
```

## ğŸ›ï¸ CenÃ¡rios de Uso

### ğŸ—ï¸ **Desenvolvimento vs ProduÃ§Ã£o**

```bash
# Desenvolvimento: GrÃ¡tis e rÃ¡pido
OLLAMA_MODEL=llama2
PREFERRED_LLM_PROVIDER=ollama

# ProduÃ§Ã£o: MÃ¡xima qualidade
OPENAI_API_KEY=sk-prod-key
PREFERRED_LLM_PROVIDER=openai
```

### ğŸ” **Dados SensÃ­veis**

```bash
# Apenas modelos internos/locais
OLLAMA_MODEL=llama2
CUSTOM_LLM_API_BASE=https://internal-api.company.com
PREFERRED_LLM_PROVIDER=custom
PREFERRED_EMBEDDING_PROVIDER=ollama
```

### ğŸ’° **OtimizaÃ§Ã£o de Custos**

```bash
# LLM local (barato) + Embeddings OpenAI (precisos)
OLLAMA_MODEL=llama2
OPENAI_API_KEY=sk-key
PREFERRED_LLM_PROVIDER=ollama
PREFERRED_EMBEDDING_PROVIDER=openai
```

### ğŸš€ **Alta Disponibilidade**

```bash
# Configure mÃºltiplos provedores para fallback
OPENAI_API_KEY=sk-key
OLLAMA_MODEL=llama2
CUSTOM_LLM_API_BASE=https://backup-api.com
# Sistema alterna automaticamente se um falhar
```

## ğŸ—ï¸ Arquitetura

```mermaid
graph TB
    A[Multi-LLM RAG System] --> B[Provider Manager]
    B --> C[OpenAI Provider]
    B --> D[Ollama Provider]
    B --> E[Custom Provider]
    
    A --> F[Database Scanner]
    A --> G[Vector Store Manager]
    A --> H[SQL Agent]
    
    F --> I[Schema Analysis]
    G --> J[FAISS Index]
    H --> K[Query Generation]
    
    C --> L[GPT-4/3.5]
    D --> M[Llama2/Mistral]
    E --> N[Company APIs]
```

## ğŸ“Š Funcionalidades AvanÃ§adas

### ğŸ”„ **Provider Management**
- Auto-detecÃ§Ã£o de provedores disponÃ­veis
- Fallback inteligente
- Health checks automÃ¡ticos
- MÃ©tricas de performance

### ğŸ›¡ï¸ **SeguranÃ§a**
- ProteÃ§Ã£o SQL injection
- SanitizaÃ§Ã£o de queries
- Headers customizados
- Logs de auditoria

### âš¡ **Performance**
- Cache de embeddings
- Pool de conexÃµes
- Retry automÃ¡tico
- Timeout configurÃ¡vel

### ğŸ“ˆ **Monitoramento**
- Status de todos os provedores
- MÃ©tricas de uso
- Logs estruturados
- Health endpoints

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### ğŸŒ **Headers Customizados**

```bash
# Para APIs empresariais com autenticaÃ§Ã£o especÃ­fica
CUSTOM_LLM_HEADER_X_API_VERSION=2023-12-01
CUSTOM_LLM_HEADER_X_TENANT_ID=sua-empresa
CUSTOM_LLM_HEADER_Authorization=Bearer custom-token
```

### ğŸ¯ **Formatos de API**

```bash
# Para APIs compatÃ­veis com OpenAI
CUSTOM_LLM_FORMAT=openai
CUSTOM_LLM_ENDPOINT=/v1/chat/completions

# Para APIs estilo Ollama
CUSTOM_LLM_FORMAT=ollama
CUSTOM_LLM_ENDPOINT=/api/generate
```

### ğŸ“ **Logs Detalhados**

```bash
# ConfiguraÃ§Ã£o de logging
LOG_LEVEL=DEBUG
ENABLE_PROVIDER_LOGGING=true
LOG_FILE=./logs/multi_llm.log
```

## ğŸ” API Endpoints

### ğŸ“‹ **Consultas**
- `POST /query` - Consulta principal
- `POST /query?provider=ollama` - Consulta com provedor especÃ­fico

### ğŸ”„ **Gerenciamento**
- `POST /switch-provider` - Trocar provedor ativo
- `GET /providers` - Listar todos os provedores
- `GET /status` - Status completo do sistema

### ğŸ©º **Monitoramento**
- `GET /health` - Health check
- `GET /metrics` - MÃ©tricas de uso

## ğŸ§ª Testes

```bash
# Teste rÃ¡pido do sistema
python quick_test.py

# Testes unitÃ¡rios
pytest tests/unit/

# Testes de integraÃ§Ã£o
pytest tests/integration/

# Teste de performance
python tests/performance/benchmark_providers.py

# Exemplo completo
python examples/multi_llm_usage.py
```

## ğŸ“š DocumentaÃ§Ã£o

- ğŸ“– **[Guia Completo Multi-LLM](docs/MULTI_LLM_GUIDE.md)** - DocumentaÃ§Ã£o detalhada
- ğŸ”§ **[ConfiguraÃ§Ã£o AvanÃ§ada](docs/ADVANCED_CONFIG.md)** - OpÃ§Ãµes avanÃ§adas
- ğŸ¢ **[Deploy Empresarial](docs/ENTERPRISE_DEPLOY.md)** - Deploy em produÃ§Ã£o
- ğŸ” **[SeguranÃ§a](docs/SECURITY.md)** - PrÃ¡ticas de seguranÃ§a

## ğŸ†š ComparaÃ§Ã£o com Sistema Original

| Funcionalidade | Sistema Original | Multi-LLM System |
|---|---|---|
| **Provedores LLM** | OpenAI apenas | OpenAI + Ollama + Custom |
| **Flexibilidade** | Limitada | Total |
| **Dados SensÃ­veis** | Na nuvem | Local disponÃ­vel |
| **Custos** | Fixos (OpenAI) | OtimizÃ¡veis |
| **Fallback** | NÃ£o | AutomÃ¡tico |
| **APIs Empresariais** | NÃ£o | Sim |
| **Desenvolvimento Local** | Requer API key | Gratuito com Ollama |

## ğŸ‰ PrÃ³ximos Passos

1. **âœ… Execute o teste rÃ¡pido**: `python quick_test.py`
2. **ğŸ”§ Configure seus provedores** no arquivo `.env`
3. **ğŸš€ Inicie a API**: `python multi_llm_api.py`
4. **ğŸ§ª Teste via navegador**: http://localhost:9000/docs
5. **ğŸ“– Leia o guia completo**: [docs/MULTI_LLM_GUIDE.md](docs/MULTI_LLM_GUIDE.md)

## ğŸ¤ ContribuiÃ§Ãµes

- **ğŸ› Issues**: Reporte bugs e problemas
- **ğŸ’¡ Features**: Sugira novas funcionalidades
- **ğŸ“ Docs**: Melhore a documentaÃ§Ã£o
- **ğŸ§ª Tests**: Adicione mais testes

---

### ğŸ¯ **Resultado Final**

Um sistema **flexÃ­vel**, **seguro** e **pronto para produÃ§Ã£o** que se adapta desde desenvolvimento local atÃ© APIs empresariais proprietÃ¡rias!

**ğŸš€ Comece agora**: `python quick_test.py`
