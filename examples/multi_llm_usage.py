"""
Multi-LLM RAG System Usage Examples

This script demonstrates how to use the enhanced RAG system with multiple LLM providers
including OpenAI, Ollama (local models), and custom enterprise APIs.
"""
import os
import sys

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from src.multi_llm_rag_system import MultiLLMDatabaseRAGSystem
from src.config_multi_llm import DatabaseConfig, MultiLLMConfig, RAGConfig
from src.llm_providers.provider_manager import LLMProviderManager
from src.llm_providers import LLMProvider, LLMConfig, EmbeddingConfig


def setup_environment_examples():
    """Show different environment setups"""
    print("=" * 60)
    print("üîß ENVIRONMENT SETUP EXAMPLES")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ OpenAI Only Setup:")
    print("   export OPENAI_API_KEY='your-key-here'")
    print("   export OPENAI_MODEL='gpt-4'")
    print("   export OPENAI_EMBEDDING_MODEL='text-embedding-ada-002'")
    
    print("\n2Ô∏è‚É£ Ollama (Local) Only Setup:")
    print("   export OLLAMA_MODEL='llama2'")
    print("   export OLLAMA_EMBEDDING_MODEL='llama2'")
    print("   export OLLAMA_BASE_URL='http://localhost:11434'")
    print("   # Make sure Ollama is running: ollama serve")
    
    print("\n3Ô∏è‚É£ Enterprise/Custom API Setup:")
    print("   export CUSTOM_LLM_API_BASE='https://your-company-api.com'")
    print("   export CUSTOM_LLM_MODEL='your-model'")
    print("   export CUSTOM_LLM_API_KEY='your-enterprise-key'")
    print("   export CUSTOM_LLM_FORMAT='openai'")
    
    print("\n4Ô∏è‚É£ Mixed Setup (Multiple Providers):")
    print("   # Configure multiple providers and set preferences")
    print("   export PREFERRED_LLM_PROVIDER='openai'")
    print("   export PREFERRED_EMBEDDING_PROVIDER='custom'")
    print("   # + any combination of the above")


def demo_basic_usage():
    """Demonstrate basic multi-LLM usage"""
    print("\n" + "=" * 60)
    print("üöÄ BASIC USAGE DEMO")
    print("=" * 60)
    
    try:
        # Method 1: Create from environment variables
        print("\nüìã Method 1: Create from Environment")
        from src.multi_llm_rag_system import create_multi_llm_rag_system_from_env
        
        rag_system = create_multi_llm_rag_system_from_env()
        
        if rag_system.initialize():
            print("‚úÖ System initialized successfully!")
            
            # Example query
            result = rag_system.query("How many users are in the database?")
            print(f"\nü§î Query: How many users are in the database?")
            print(f"üí° Answer: {result.get('answer', 'No answer')}")
            
            if result.get('error'):
                print(f"‚ö†Ô∏è Error: {result['error']}")
        else:
            print("‚ùå Failed to initialize system")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("\nüí° Make sure you have at least one LLM provider configured!")


def demo_provider_switching():
    """Demonstrate switching between providers"""
    print("\n" + "=" * 60)
    print("üîÑ PROVIDER SWITCHING DEMO")
    print("=" * 60)
    
    # This would work if you have multiple providers configured
    try:
        from src.multi_llm_rag_system import create_multi_llm_rag_system_from_env
        
        rag_system = create_multi_llm_rag_system_from_env()
        
        if rag_system.initialize():
            print("‚úÖ System initialized!")
            
            # Show current status
            status = rag_system.get_system_info()
            print(f"\nüìä Available Providers:")
            for provider, info in status["providers"]["llm_providers"].items():
                active = " [ACTIVE]" if info["active"] else ""
                available = "‚úÖ" if info["available"] else "‚ùå"
                print(f"   {available} {provider}{active}")
            
            print(f"\nüîÑ Trying to switch providers...")
            
            # Try switching to different providers
            providers_to_try = ["ollama", "openai", "custom"]
            for provider in providers_to_try:
                success = rag_system.switch_llm_provider(provider)
                if success:
                    print(f"‚úÖ Switched to {provider}")
                    
                    # Test a query with the new provider
                    result = rag_system.query("What tables exist in this database?")
                    print(f"   ü§î Query result: {result.get('answer', 'No answer')[:100]}...")
                    break
                else:
                    print(f"‚ùå Could not switch to {provider}")
        else:
            print("‚ùå Failed to initialize system")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


def demo_manual_configuration():
    """Demonstrate manual provider configuration"""
    print("\n" + "=" * 60)
    print("‚öôÔ∏è MANUAL CONFIGURATION DEMO")
    print("=" * 60)
    
    try:
        # Create configurations manually
        db_config = DatabaseConfig(
            url="sqlite:///./sample_data.db",
            type="sqlite"
        )
        
        # Create provider manager manually
        manager = LLMProviderManager()
        
        print("üîß Adding providers manually...")
        
        # Add OpenAI if key is available
        if os.getenv("OPENAI_API_KEY"):
            openai_config = LLMConfig(
                provider=LLMProvider.OPENAI,
                model_name="gpt-4",
                api_key=os.getenv("OPENAI_API_KEY"),
                temperature=0.0
            )
            manager.add_llm_provider(LLMProvider.OPENAI, openai_config)
            
            openai_embedding_config = EmbeddingConfig(
                provider=LLMProvider.OPENAI,
                model_name="text-embedding-ada-002",
                api_key=os.getenv("OPENAI_API_KEY")
            )
            manager.add_embedding_provider(LLMProvider.OPENAI, openai_embedding_config)
            print("   ‚úÖ OpenAI configured")
        else:
            print("   ‚è≠Ô∏è OpenAI skipped (no API key)")
        
        # Add Ollama if available
        if os.getenv("OLLAMA_MODEL"):
            ollama_config = LLMConfig(
                provider=LLMProvider.OLLAMA,
                model_name=os.getenv("OLLAMA_MODEL", "llama2"),
                api_base="http://localhost:11434"
            )
            manager.add_llm_provider(LLMProvider.OLLAMA, ollama_config)
            print("   ‚úÖ Ollama configured")
        else:
            print("   ‚è≠Ô∏è Ollama skipped (no model specified)")
        
        # Create RAG system with manual config
        multi_llm_config = MultiLLMConfig.from_env()  # Still use env for other settings
        rag_config = RAGConfig.from_env()
        
        rag_system = MultiLLMDatabaseRAGSystem(
            db_config=db_config,
            multi_llm_config=multi_llm_config,
            rag_config=rag_config,
            provider_manager=manager
        )
        
        if rag_system.initialize():
            print("‚úÖ Manually configured system initialized!")
            
            # Show system status
            print(f"\nüìä System Status:")
            status = rag_system.get_system_info()
            print(f"   üóÑÔ∏è Database: {status['database_config']['type']}")
            print(f"   üîç Vector store: {'Ready' if status['initialized'] else 'Not ready'}")
            
        else:
            print("‚ùå Failed to initialize manually configured system")
            
    except Exception as e:
        print(f"‚ùå Error in manual configuration: {e}")


def demo_api_usage():
    """Show how to use the multi-LLM API"""
    print("\n" + "=" * 60)
    print("üåê API USAGE EXAMPLES")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ Start the Multi-LLM API:")
    print("   python multi_llm_api.py")
    print("   # API will be available at http://localhost:9000")
    
    print("\n2Ô∏è‚É£ Basic Query (curl):")
    print("""   curl -X POST http://localhost:9000/query \\
     -H "Authorization: Bearer dev-multi-llm-key-12345" \\
     -H "Content-Type: application/json" \\
     -d '{"query": "How many users are there?"}'""")
    
    print("\n3Ô∏è‚É£ Query with Provider Override:")
    print("""   curl -X POST http://localhost:9000/query \\
     -H "Authorization: Bearer dev-multi-llm-key-12345" \\
     -H "Content-Type: application/json" \\
     -d '{"query": "Show user statistics", "provider": "ollama"}'""")
    
    print("\n4Ô∏è‚É£ Check System Status:")
    print("""   curl -X GET http://localhost:9000/status \\
     -H "Authorization: Bearer dev-multi-llm-key-12345\"""")
    
    print("\n5Ô∏è‚É£ Switch Provider:")
    print("""   curl -X POST http://localhost:9000/switch-provider \\
     -H "Authorization: Bearer dev-multi-llm-key-12345" \\
     -H "Content-Type: application/json" \\
     -d '{"provider": "ollama", "type": "llm"}'""")


def main():
    """Main demonstration"""
    print("üéØ MULTI-LLM RAG SYSTEM DEMONSTRATIONS")
    print("=" * 60)
    print("This system supports:")
    print("  ü§ñ OpenAI (GPT models)")
    print("  üè† Ollama (Local models like Llama, Mistral, etc.)")
    print("  üè¢ Custom Enterprise APIs")
    print("  üîÑ Dynamic provider switching")
    print("  üìä Comprehensive monitoring")
    
    # Show environment setup examples
    setup_environment_examples()
    
    # Run demos based on what's available
    print(f"\nüîç Checking available providers...")
    
    providers_available = []
    if os.getenv("OPENAI_API_KEY"):
        providers_available.append("OpenAI")
    if os.getenv("OLLAMA_MODEL"):
        providers_available.append("Ollama")
    if os.getenv("CUSTOM_LLM_API_BASE"):
        providers_available.append("Custom")
    
    if providers_available:
        print(f"‚úÖ Found providers: {', '.join(providers_available)}")
        
        # Run live demos
        demo_basic_usage()
        demo_provider_switching()
        demo_manual_configuration()
    else:
        print("‚ö†Ô∏è No providers configured. Please set up at least one provider to run demos.")
        print("üìñ See .env.multi-llm.example for configuration examples")
    
    # Always show API usage (doesn't require live providers)
    demo_api_usage()
    
    print(f"\nüéâ Demo complete! Configure providers and try the examples above.")


if __name__ == "__main__":
    main()
