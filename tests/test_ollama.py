#!/usr/bin/env python3
"""
üß™ TESTE DO PROVEDOR OLLAMA
===========================

Script para verificar se o servidor Ollama est√° ativo e funcionando corretamente.
Execute: python test_ollama.py
"""

import os
import sys
import requests
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Adicionar o src ao path - ajustado para nova estrutura
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

def load_env():
    """Carregar vari√°veis de ambiente do .env"""
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("‚úì Arquivo .env carregado")
        return True
    except ImportError:
        print("‚ö†Ô∏è python-dotenv n√£o instalado")
        print("   As vari√°veis de ambiente do sistema ser√£o usadas")
        return False

def get_ollama_config() -> Dict[str, str]:
    """Obter configura√ß√£o do Ollama a partir das vari√°veis de ambiente"""
    return {
        'base_url': os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434'),
        'model': os.getenv('OLLAMA_MODEL', 'llama2'),
        'embedding_model': os.getenv('OLLAMA_EMBEDDING_MODEL', 'llama2')
    }

def test_ollama_health(base_url: str) -> bool:
    """Testar se o servidor Ollama est√° rodando"""
    try:
        # Remover barra final se existir
        base_url = base_url.rstrip('/')
        
        print(f"üîç Testando conex√£o com: {base_url}")
        
        # Testar endpoint b√°sico
        response = requests.get(f"{base_url}/api/tags", timeout=10)
        
        if response.status_code == 200:
            print("‚úÖ Servidor Ollama est√° ativo!")
            return True
        else:
            print(f"‚ùå Servidor respondeu com c√≥digo: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("‚ùå N√£o foi poss√≠vel conectar ao servidor Ollama")
        print(f"   Verifique se o servidor est√° rodando em: {base_url}")
        return False
    except requests.exceptions.Timeout:
        print("‚è±Ô∏è Timeout ao conectar com o servidor")
        return False
    except Exception as e:
        print(f"‚ùå Erro inesperado: {e}")
        return False

def list_ollama_models(base_url: str) -> Optional[list]:
    """Listar modelos dispon√≠veis no Ollama"""
    try:
        base_url = base_url.rstrip('/')
        response = requests.get(f"{base_url}/api/tags", timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            print(f"\nüìã Modelos dispon√≠veis ({len(models)} encontrados):")
            
            if not models:
                print("   ‚ö†Ô∏è Nenhum modelo encontrado!")
                print("   üí° Para instalar um modelo, execute:")
                print("      ollama pull llama2")
                return []
            
            for i, model in enumerate(models, 1):
                name = model.get('name', 'N/A')
                size = model.get('size', 0)
                size_mb = round(size / (1024 * 1024), 1) if size else 0
                modified = model.get('modified_at', 'N/A')
                
                print(f"   {i}. üì¶ {name}")
                print(f"      üíæ Tamanho: {size_mb} MB")
                print(f"      üïí Modificado: {modified[:19] if modified != 'N/A' else 'N/A'}")
                print()
            
            return [model.get('name') for model in models]
            
        else:
            print(f"‚ùå Erro ao listar modelos: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"‚ùå Erro ao listar modelos: {e}")
        return None

def test_model_availability(base_url: str, model_name: str, available_models: list) -> bool:
    """Verificar se um modelo espec√≠fico est√° dispon√≠vel"""
    if not available_models:
        return False
    
    # Verificar se o modelo est√° na lista (com ou sem :latest)
    model_variants = [model_name, f"{model_name}:latest"]
    
    found_model = None
    for variant in model_variants:
        if variant in available_models:
            found_model = variant
            break
    
    if found_model:
        print(f"‚úÖ Modelo '{model_name}' est√° dispon√≠vel como '{found_model}'")
        return True
    else:
        print(f"‚ùå Modelo '{model_name}' n√£o encontrado!")
        print(f"   üí° Para instalar, execute: ollama pull {model_name}")
        return False

def test_ollama_generation(base_url: str, model_name: str) -> bool:
    """Testar gera√ß√£o de texto com o modelo"""
    try:
        base_url = base_url.rstrip('/')
        
        print(f"\nüß™ Testando gera√ß√£o de texto com '{model_name}'...")
        
        payload = {
            "model": model_name,
            "prompt": "Ol√°! Responda apenas 'Ollama funcionando!' em portugu√™s.",
            "stream": False
        }
        
        response = requests.post(
            f"{base_url}/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            generated_text = data.get('response', '').strip()
            
            print("‚úÖ Gera√ß√£o de texto funcionando!")
            print(f"üìù Resposta: {generated_text}")
            
            # Verificar se a resposta parece v√°lida
            if len(generated_text) > 0:
                print("‚úÖ Modelo est√° respondendo corretamente!")
                return True
            else:
                print("‚ö†Ô∏è Modelo n√£o gerou resposta v√°lida")
                return False
                
        else:
            print(f"‚ùå Erro na gera√ß√£o: {response.status_code}")
            try:
                error_data = response.json()
                print(f"   Detalhes: {error_data}")
            except:
                print(f"   Resposta: {response.text[:200]}...")
            return False
            
    except requests.exceptions.Timeout:
        print("‚è±Ô∏è Timeout durante gera√ß√£o (modelo pode estar carregando...)")
        return False
    except Exception as e:
        print(f"‚ùå Erro durante gera√ß√£o: {e}")
        return False

def test_ollama_provider_integration():
    """Testar integra√ß√£o com nosso provider personalizado"""
    try:
        print("\nüîå Testando integra√ß√£o com nosso Provider...")
        
        from llm_providers.ollama_provider import OllamaProvider
        from config_multi_llm import OllamaConfig
        
        config = OllamaConfig.from_env()
        
        if not config:
            print("‚ùå Configura√ß√£o Ollama n√£o encontrada no .env")
            return False
        
        provider = OllamaProvider(config)
        
        # Testar se consegue inicializar
        if provider.is_available():
            print("‚úÖ Provider Ollama inicializado com sucesso!")
            
            # Testar gera√ß√£o via provider
            try:
                result = provider.generate("Diga apenas 'Provider funcionando!'")
                print(f"üìù Resposta via provider: {result}")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è Provider criado mas erro na gera√ß√£o: {e}")
                return False
        else:
            print("‚ùå Provider Ollama n√£o est√° dispon√≠vel")
            return False
            
    except ImportError as e:
        print(f"‚ùå Erro de importa√ß√£o: {e}")
        print("   Verifique se os m√≥dulos est√£o no lugar correto")
        return False
    except Exception as e:
        print(f"‚ùå Erro no provider: {e}")
        return False

def main():
    """Fun√ß√£o principal"""
    print("üß™ TESTE COMPLETO DO PROVEDOR OLLAMA")
    print("=" * 50)
    
    # Carregar configura√ß√µes
    load_env()
    config = get_ollama_config()
    
    print(f"\nüìã Configura√ß√£o atual:")
    print(f"   üåê Base URL: {config['base_url']}")
    print(f"   ü§ñ Modelo LLM: {config['model']}")
    print(f"   üìä Modelo Embedding: {config['embedding_model']}")
    
    # Teste 1: Conectividade
    print(f"\n{'='*20} TESTE 1: CONECTIVIDADE {'='*20}")
    if not test_ollama_health(config['base_url']):
        print("\n‚ùå Servidor Ollama n√£o est√° acess√≠vel!")
        print("\nüîß Solu√ß√µes poss√≠veis:")
        print("   1. Verifique se o Ollama est√° instalado:")
        print("      https://ollama.ai")
        print("   2. Inicie o servidor:")
        print("      ollama serve")
        print("   3. Verifique a URL no .env:")
        print(f"      OLLAMA_BASE_URL={config['base_url']}")
        return False
    
    # Teste 2: Modelos dispon√≠veis
    print(f"\n{'='*20} TESTE 2: MODELOS {'='*20}")
    available_models = list_ollama_models(config['base_url'])
    
    if available_models is None:
        print("‚ùå N√£o foi poss√≠vel listar modelos")
        return False
    
    # Teste 3: Modelo configurado
    print(f"\n{'='*20} TESTE 3: MODELO CONFIGURADO {'='*20}")
    model_available = test_model_availability(
        config['base_url'], 
        config['model'], 
        available_models
    )
    
    # Teste 4: Gera√ß√£o de texto
    if model_available:
        print(f"\n{'='*20} TESTE 4: GERA√á√ÉO DE TEXTO {'='*20}")
        generation_ok = test_ollama_generation(config['base_url'], config['model'])
    else:
        print(f"\n{'='*20} TESTE 4: PULADO (modelo n√£o dispon√≠vel) {'='*20}")
        generation_ok = False
    
    # Teste 5: Integra√ß√£o com nosso provider
    print(f"\n{'='*20} TESTE 5: INTEGRA√á√ÉO PROVIDER {'='*20}")
    provider_ok = test_ollama_provider_integration()
    
    # Resumo final
    print(f"\n{'='*20} RESUMO FINAL {'='*20}")
    
    tests = [
        ("üåê Conectividade", True),
        ("üìã Listagem de modelos", available_models is not None),
        ("ü§ñ Modelo configurado", model_available),
        ("üß™ Gera√ß√£o de texto", generation_ok),
        ("üîå Provider integra√ß√£o", provider_ok)
    ]
    
    passed = sum(1 for _, result in tests if result)
    total = len(tests)
    
    for test_name, result in tests:
        status = "‚úÖ PASSOU" if result else "‚ùå FALHOU"
        print(f"   {test_name}: {status}")
    
    print(f"\nüìä Resultado: {passed}/{total} testes passaram")
    
    if passed == total:
        print("üéâ OLLAMA EST√Å TOTALMENTE FUNCIONAL!")
        print("\n‚úÖ Voc√™ pode usar o provedor Ollama no sistema Multi-LLM")
        print("   Para usar como preferido, configure no .env:")
        print("   PREFERRED_LLM_PROVIDER=ollama")
    elif passed >= 3:
        print("‚ö†Ô∏è OLLAMA PARCIALMENTE FUNCIONAL")
        print("   Alguns testes falharam, mas o b√°sico est√° funcionando")
    else:
        print("‚ùå OLLAMA N√ÉO EST√Å FUNCIONAL")
        print("   Verifique a instala√ß√£o e configura√ß√£o")
    
    return passed == total

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Teste interrompido pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro inesperado: {e}")
        sys.exit(1)
